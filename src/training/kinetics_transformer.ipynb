{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f205ea0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import clip\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import webdataset as wds\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d230dcf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: clip in ./.local/lib/python3.8/site-packages (0.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.8/site-packages (1.12.0+cu113)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.8/site-packages (0.13.0+cu113)\n",
      "Requirement already satisfied: torchaudio in ./.local/lib/python3.8/site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from torchvision) (1.23.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.8/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.8/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.8/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: webdataset in ./.local/lib/python3.8/site-packages (0.2.5)\n",
      "Requirement already satisfied: braceexpand in ./.local/lib/python3.8/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from webdataset) (1.23.0)\n",
      "Requirement already satisfied: pyyaml in ./.local/lib/python3.8/site-packages (from webdataset) (6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./.local/lib/python3.8/site-packages (from pandas) (1.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in ./.local/lib/python3.8/site-packages (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3.8 install clip\n",
    "!pip3.8 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip3.8 install webdataset\n",
    "!pip3.8 install pandas\n",
    "!pip3.8 install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba27af",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "793e2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset code:\n",
    "\"\"\"\n",
    "utils for processing datasets of format described in https://github.com/iejMac/clip-video-encode/pull/13\n",
    "used https://github.com/rom1504/laion-prepro/blob/main/laion5B/usage_guide/dataloader_pytorch.py as template\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def standardize_embedding_shape(emb, seq_len):\n",
    "    if len(emb) > seq_len:\n",
    "        print(f\"Warning: Raw embedding is longer than standard sequence length ({len(emb)} > {seq_len})\")\n",
    "        emb = emb[:seq_len]\n",
    "\n",
    "    pad = np.zeros((seq_len - len(emb), emb.shape[1]), dtype=emb.dtype)\n",
    "    padded_emb = np.concatenate([emb, pad])\n",
    "    return padded_emb\n",
    "\n",
    "\n",
    "def create_embeddingwebdataset(\n",
    "    urls,\n",
    "    embedding_transform=lambda emb: emb,\n",
    "    standard_seq_len=-1,\n",
    "    to_tensor=True,\n",
    "    enable_text=True,\n",
    "    enable_meta=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a WebDataset reader for Frame Embedding Dataset\n",
    "    Input:\n",
    "        standard_seq_len: sequence length to pad all embedding sequences to (for batching)\n",
    "            !(-1) : pad to standard_seq_len\n",
    "            -1: don't pad (dataset can't be used in DataLoader with batch_size > 1)\n",
    "        enable_text: include text captions\n",
    "        enable_meta: include metadata\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = wds.WebDataset(urls)\n",
    "    # TODO: different tokeinzers??\n",
    "    tokenizer = lambda text: clip.tokenize([text], truncate=True)[0]\n",
    "\n",
    "    def preprocess_dataset(item):\n",
    "        output = {}\n",
    "\n",
    "        npy_data = item[\"npy\"]\n",
    "        stream = io.BytesIO(npy_data)\n",
    "        emb = np.lib.format.read_array(stream)\n",
    "\n",
    "        if standard_seq_len != -1:\n",
    "            emb = standardize_embedding_shape(emb, standard_seq_len)\n",
    "        if to_tensor:\n",
    "            emb = torch.from_numpy(emb)\n",
    "\n",
    "        output[\"embeddings\"] = embedding_transform(emb)\n",
    "\n",
    "        if enable_text:\n",
    "            text_data = item[\"cap\"]\n",
    "            text = text_data.decode(\"utf-8\")\n",
    "            output[\"text\"] = text\n",
    "            output[\"text_tokens\"] = tokenizer(text)\n",
    "        if enable_meta:\n",
    "            meta_data = item[\"json\"]\n",
    "            meta = meta_data.decode(\"utf-8\")\n",
    "            output[\"meta\"] = meta\n",
    "        return output\n",
    "\n",
    "    transformed_dataset = dataset.map(preprocess_dataset, handler=wds.handlers.warn_and_continue)\n",
    "    return transformed_dataset\n",
    "\n",
    "\n",
    "def dataset_to_dataloader(dataset, batch_size, num_prepro_workers):\n",
    "    \"\"\"converts WebDataset to PyTorch DataLoader.\"\"\"\n",
    "\n",
    "    dl = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_prepro_workers,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return dl\n",
    "\n",
    "\n",
    "class EmbeddingWebDatasetReader:\n",
    "    \"\"\"WebDataset reader for Embedding Datasets\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        urls,\n",
    "        standard_seq_len,\n",
    "        batch_size,\n",
    "        num_prepro_workers,\n",
    "        to_tensor=True,\n",
    "        enable_text=True,\n",
    "        enable_meta=False,\n",
    "        embedding_transform=lambda emb: emb,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        dataset = create_embeddingwebdataset(\n",
    "            urls,\n",
    "            embedding_transform,\n",
    "            standard_seq_len,\n",
    "            to_tensor,\n",
    "            enable_text,\n",
    "            enable_meta,\n",
    "        )\n",
    "        self.dataloader = dataset_to_dataloader(dataset, batch_size, num_prepro_workers)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2bef7",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09730dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Positional Encodings\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Transformer for encoding sequences of frame embeddings\n",
    "\"\"\"\n",
    "\n",
    "# source - https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class VideoEmbeddingTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, proj_dim=None, dropout = 0.):\n",
    "      super().__init__()\n",
    "      self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "      self.pos_encoding = PositionalEncoding(dim, dropout)\n",
    "\n",
    "      self.proj = None if proj_dim is None else nn.Sequential(\n",
    "          nn.Linear(dim, (dim+proj_dim)//2),\n",
    "          nn.GELU(),\n",
    "          nn.Linear((dim+proj_dim)//2, proj_dim),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.pos_encoding(x)\n",
    "      x = self.transformer(x)\n",
    "\n",
    "      x = x[..., 0, :] # first embed = video embedding\n",
    "\n",
    "      if self.proj is not None:\n",
    "        x = self.proj(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a1e80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/iejmac/wds_kinetics\"\n",
    "splits = pd.read_csv(os.path.join(DATA_DIR, \"splits.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7b23d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tars = splits[splits[\"split\"] == \"train\"][\"tar_file\"].tolist()\n",
    "val_tars = splits[splits[\"split\"] == \"val\"][\"tar_file\"].tolist()\n",
    "\n",
    "train_tars_paths = [os.path.join(DATA_DIR, t + \".tar\") for t in train_tars]\n",
    "val_tars_paths = [os.path.join(DATA_DIR, t + \".tar\") for t in val_tars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "724fd4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = pd.read_csv(os.path.join(DATA_DIR, \"annotations/train.csv\"))[\"label\"].unique().tolist()\n",
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d0d3d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c153a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PARAMS:\n",
    "DIM = 512\n",
    "DEPTH = 12\n",
    "HEADS = 8\n",
    "DIM_HEAD = 128\n",
    "MLP_DIM = 512\n",
    "PROJ_DIM = 700\n",
    "DROPOUT=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e0bfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = VideoEmbeddingTransformer(\n",
    "    dim=DIM,\n",
    "    depth=DEPTH,\n",
    "    heads=HEADS,\n",
    "    dim_head=DIM_HEAD,\n",
    "    mlp_dim=MLP_DIM,\n",
    "    proj_dim=PROJ_DIM,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tf = tf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8277b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from open_clip/training/scheduler.py\n",
    "def assign_learning_rate(optimizer, new_lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = new_lr\n",
    "\n",
    "\n",
    "def _warmup_lr(base_lr, warmup_length, step):\n",
    "    return base_lr * (step + 1) / warmup_length\n",
    "\n",
    "\n",
    "def cosine_lr(optimizer, base_lr, warmup_length, steps):\n",
    "    def _lr_adjuster(step):\n",
    "        if step < warmup_length:\n",
    "            lr = _warmup_lr(base_lr, warmup_length, step)\n",
    "        else:\n",
    "            e = step - warmup_length\n",
    "            es = steps - warmup_length\n",
    "            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * base_lr\n",
    "        assign_learning_rate(optimizer, lr)\n",
    "        return lr\n",
    "    return _lr_adjuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f92e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "GRAD_CLIP = 1.0\n",
    "LAMBDA = 0.8\n",
    "EPOCHS = 20\n",
    "\n",
    "WARMUP_STEPS = 1000\n",
    "ALL_STEPS = 120000\n",
    "\n",
    "SEQ_LEN = 25\n",
    "BATCH_SIZE = 128\n",
    "NUM_PREPRO = 6\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7657b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(tf.parameters(), lr=LR, weight_decay=0.0)\n",
    "\n",
    "# lr_schedule = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda step: LAMBDA**step)\n",
    "lr_schedule = cosine_lr(opt, LR, WARMUP_STEPS, ALL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94f83dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reader = EmbeddingWebDatasetReader(\n",
    "    urls=val_tars_paths,\n",
    "    standard_seq_len=SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_prepro_workers=NUM_PREPRO,\n",
    "    to_tensor=True,\n",
    "    enable_text=True,\n",
    "    enable_meta=True,\n",
    "    embedding_transform=lambda emb: emb,\n",
    ")\n",
    "\n",
    "train_reader = EmbeddingWebDatasetReader(\n",
    "    urls=train_tars_paths,\n",
    "    standard_seq_len=SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_prepro_workers=NUM_PREPRO,\n",
    "    to_tensor=True,\n",
    "    enable_text=True,\n",
    "    enable_meta=False,\n",
    "    embedding_transform=lambda emb: emb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11408de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0...\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for e in range(EPOCHS):\n",
    "  print(f\"Epoch {e}...\")\n",
    "  print(\"here\")\n",
    "  for i, b in enumerate(train_reader):\n",
    "    step += 1\n",
    "    lr_schedule(step)\n",
    "    print('here')\n",
    "    embeddings = b[\"embeddings\"].float().to(device)\n",
    "    labs = torch.Tensor([all_labels.index(l) for l in b[\"text\"]]).long().to(device)\n",
    "    print('here')\n",
    "\n",
    "    opt.zero_grad()\n",
    "    print('here')\n",
    "\n",
    "    pred = tf(embeddings)\n",
    "    loss = loss_f(pred, labs)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # clip grads:\n",
    "    torch.nn.utils.clip_grad_norm_(tf.parameters(), GRAD_CLIP)\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if (step + 1) % 100 == 0:\n",
    "      print(f\"epoch {e} : step {step} average loss = {running_loss/100}\")\n",
    "      running_loss = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "532a1390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n",
      "/home/iejmac/.local/lib/python3.8/site-packages/webdataset/handlers.py:33: UserWarning: AttributeError(\"module 'clip' has no attribute 'tokenize'\")\n",
      "  warnings.warn(repr(exn))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mall\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val_b \u001b[38;5;129;01min\u001b[39;00m val_reader:\n\u001b[1;32m      5\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m val_b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m     labs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([all_labels\u001b[38;5;241m.\u001b[39mindex(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m val_b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mEmbeddingWebDatasetReader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1330\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1330\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1286\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1286\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1288\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1134\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib64/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "all_ = 0\n",
    "with torch.no_grad():\n",
    "  for val_b in val_reader:\n",
    "    embeddings = val_b[\"embeddings\"].float().to(device)\n",
    "    labs = torch.Tensor([all_labels.index(l) for l in val_b[\"text\"]])\n",
    "\n",
    "    pred = tf(embeddings).cpu()\n",
    "    pred_cls = torch.argmax(pred, axis=-1)\n",
    "\n",
    "    all_ += len(labs)\n",
    "    correct += torch.sum(labs == pred_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cdbb6d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(correct\u001b[38;5;241m/\u001b[39m\u001b[43mall_\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_' is not defined"
     ]
    }
   ],
   "source": [
    "print(correct/all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88b695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
